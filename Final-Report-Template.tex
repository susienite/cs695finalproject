\documentclass[conference]{IEEEtran}
%\documentclass[sigconf]{acmart}
\makeatletter
\def\ps@headings{%
\def\@oddhead{\mbox{}\scriptsize\rightmark \hfil \thepage}%
\def\@evenhead{\scriptsize\thepage \hfil \leftmark\mbox{}}%
\def\@oddfoot{}%
\def\@evenfoot{}}
\makeatother
\pagestyle{empty}
\usepackage{url}
\usepackage{graphicx,subfigure}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{epsfig}
\newtheorem{theorem}{Theorem}
\renewcommand{\algorithmicrequire}{\textbf{Input:}} % Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm
\usepackage{amsfonts}
%\newtheorem{theorem}{Theorem}[section]
\newtheorem{mydef}{Definition}[section]
%\newtheorem{lemma}{Lemma}[section]
\usepackage{multirow}
\usepackage{color}
\usepackage{array}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[underline=true]{pgf-umlsd}
\newcommand{\tabincell}[2]
{\begin{tabular}
		{@{}#1@{}}#2\end{tabular}}
\usepackage{setspace}
\renewcommand{\labelitemi}{$\vcenter{\hbox{\tiny$\bullet$}}$}


\hyphenation{op-tical net-works semi-conduc-tor}




\begin{document}



\title{Breast Cancer Wisconsin Dataset:\\ 
Application and Results of 6 Different Classification Algorithms}

\author{\IEEEauthorblockN{Sarah DeCelie}
\IEEEauthorblockA{\textit{Applied Machine Learning} \\
\textit{Stevens Institute of Technology}\\
Hoboken, USA \\
sdecelie1@stevens.edu}
\and
\IEEEauthorblockN{David Ogbonna}
\IEEEauthorblockA{\textit{Applied Machine Learning} \\
\textit{Stevens Institute of Technology}\\
Hoboken, USA \\
email address}
\and
\IEEEauthorblockN{Susan Tan}
\IEEEauthorblockA{\textit{Applied Machine Learning} \\
\textit{Stevens Institute of Technology}\\
Hoboken, USA \\
email address}
}

\maketitle


\begin{abstract}
One in 8 women in the United States is diagnosed with Breast Cancer. It has thus become pivotal in the field of healthcare to predict whether a cancer call is harmful as early as possible in order to treat patients sooner and save their lives. The Breast Cancer dataset, composed of 569 samples from Wisconsin, includes features of a breast cancer mass such as area, radius, texture, and whether the samples are malignant or benign. In this paper, we apply six different classification algorithms - Decision Tree Learning, Random Forest, SVM, kNN, Logistic Regression, and Naive Bayes - and compare their results to determine which performs the best finding a model that separates the malignant and benign samples.
\end{abstract}

\section{Introduction}
According to BreastCancer.org, breast cancer is the most common cancer across the globe, accounting for roughly 12.5\% of all new cancer cases worldwide. This is reflected within the United States, where an estimated 30\% of newly diagnosed cancers in women will be breast cancer (2023, para. 1). It is no secret how lethal this cancer can be, and how critical it is to be able to detect it early in order to begin treatment and save patients' lives. Of course, genetics plays a role, which is why when one woman in a family is diagnosed with breast cancer, doctors will urge others that are closely related to her to get tested as well. However, the majority of breast cancer cases occur in women with no prior family history of breast cancer - about 85\% specifically (BreastCancer.org, 2023, para. 1). With such a high incidence of mutations resulting from aging and other life processes, it becomes evident that relying solely on the diagnoses of other women in the family is not a perfect strategy. We must become better at detecting harmful breast cancer cells before they spread into the rest of the body.

For this study, we intend to apply six different classification algorithms to a dataset created by the University of Wisconsin, whose researchers took grayscale images of 569 different breast cancer cell's nuclei. This dataset consists of several features of each nucleus, and whether the cell is diagnosed malignant (M) or benign (B). Of these 569 samples, 357 were diagnosed benign, and 212 malignant. For each of these 10 features, the mean, standard error, and "worst" (mean of the three largest values) is calculated, totaling to 32 columns including the sample's unique ID and diagnosis. These ten features include the nucleus':

\begin{itemize}
	\item Radius (mean of distances from center to points on the perimeter
 	\item Texture (standard deviation of gray scale values)
  	\item Perimeter
   	\item Area
    	\item Smoothness (local variation in radius lengths)
     	\item Compactness (perimeter^2 / area - 1.0)
      	\item Concavity (severity of concave portions of the contour)
       	\item Concave Points (number of concave portions of the contour)
	\item Symmetry
 	\item Fractal Dimension ("coastline approximation" - 1)
\end{itemize}

The six algorithms that will be applied to this dataset include: Decision Tree, Random Forest, k-Neural Network (kNN), Logistic Regression, Support Vector Machine (SVM), and Naive Bayes Classifier. These algorithms have been applied in previous studies, including those that used medical data, and the results have been promising. In particular, it appears the SVM and Random Forest perform the best, with kNN performing the worst, but none have testing accuracies below 90\%. Thus, there is high confidence that these algorithms will all perform very well with this dataset, further proving the viability of using machine learning to diagnose breast cancer.

\section{Related Work}
As stated previously, these algorithms have been utilized in several different previous works, including studies that focused on medical data. One such example is \textit{Machine Learning Algorithms For Breast Cancer Prediction and Diagnosis}, which actually applied 5 of the 6 algorithms that will be used in this study (SVM, Random Forest, Decision Tree, Logistic Regression, and kNN) to the same Breast Cancer Wisconsin dataset. The goals of this study were similar to this one: predict and diagnosis breast cancer using machine learning, and determine which of the selected algorithms is the most effective. Based on the resulting confusion matrix, testing accuracy and precision for each algorithm, it was observed that SVM performed the best with an accuracy of 97.2\%. Though, it should be noted that the other algorithms were not too far off, with a minimum testing accuracy of 93.7\% (Naji et al., 2021). These results are very promising, and raises curiosity now that Random Forests will be added to the list of tested algorithms.

Another study, \textit{Diagnosis of Breast Cancer Using Random Forests}, also used the same dataset and some of the same algorithms as this study. Specifically, this study compared the accuracies of SVM, Decision Tree, Multilayer Perceptron, kNN, and Random Forest. This study actually determined that Random Forest outperforms the other algorithms, with a perfect 100\% accuracy, precision, recall, F1 score, and ROC-AUC (Minnoor and Baths, 2023). With both of these studies, kNN seemed to perform the worst, but still with a good accuracy of around 93 to 94 percent.

The first study noted the fact that the use of one dataset, centering around breast cancer cells only, is a potential limitation. The second study added that the Wisconsin Breast Cancer dataset consists of calculations made from images taken of the cancerous cells' nuclei, and that developments in image processing may improve the performance of the algorithms with more accurate data. These drawbacks of using this dataset should be kept in mind, and the potential impacts on the results obtained later in this study will be discussed further.

\section{Our Solution}
This section elaborates your solution to the problem.

\subsection{Description of Dataset}
(Delete this sentence later)  this subsection is very important. It shall not only describe the source of dataset, but also provide some insights into the data, e.g., visualization of the dataset. More importantly, you are expected to explain how you pre-process the data in this subsection.
The Breast Cancer Wisconsin dataset is sourced from kaggle.com. It contains 569 samples: 357 benign and 212 malignant samples. Each sample is identified by an ID number and categorized as either M for malignant or B for benign. Ten real-valued features are computed for each cell nucleus/ sample: 
\begin{itemize}
	\item Radius (mean of distances from center to points on the perimeter
 	\item Texture (standard deviation of gray scale values)
  	\item Perimeter
   	\item Area
    	\item Smoothness (local variation in radius lengths)
     	\item Compactness (perimeter^2 / area - 1.0)
      	\item Concavity (severity of concave portions of the contour)
       	\item Concave Points (number of concave portions of the contour)
	\item Symmetry
 	\item Fractal Dimension ("coastline approximation" - 1)
\end{itemize}
As mentioned above, for each of the 10 feature above, the mean, standard error, and worst is calculated, totaling to 32 columns. Before applying machine learning algorithms to process this dataset and see how accurate each machine learning algorithm is at classifying a sample as benign or malignant, we first pre-processed the data. We read the csv file and got rid of the last column "Unnamed:32". We checked for NaN values, which apart for "Unnamed:32" column, this dataset did not have. After ensuring that the dataset contains only non-null values, we got rid of id column and separated the data into x (everything but diagnosis) and y(diagnosis). The data was then ready to be processed by our machine learning algorithms.

\subsection{Machine Learning Algorithms}
(delete later) This subsection describes machine learning algorithms that you plan to use. For each ML algorithm, briefly 1) explain why it might be appropriate for the problem and 2) describe your main design. For example, if it is neural network, provide the network structure and your initial choice of some key parameters (e.g., activation function to use, number of layers, number of hidden nodes of each layer). You may change the parameters during the training process.  


\subsection{Implementation Details}
This subsection describes details of your implementation. Please focus on how you test and validate the performance, tune the hyperparameters, and select the best-performing models. Elaborate on techniques that you apply to improve the performance and explain why you use these techniques. You include few most important results/figures to illustrate your idea but do not let figures/tables dominate the content of the report. You can include few lines of critical code if needed. But please avoid paste lengthy code in your report. Please make sure the figures/tables/code snapshots are of appropriate size including the font size.

\section{Comparison}  
This section includes the following: 1) comparing the performance of different machine learning algorithms that you used, and 2) comparing the performance of your algorithms with existing solutions if any. Please provide insights to reason about why this algorithm is better/worse than another one.

\section{Future Directions}
This section lays out some potential directions for further improving the performance. You can image what you may do if you were given extra 3-6 months.

\section{Conclusion}
This section summarizes this project, i.e., by the extensive experiments and analysis, do you think the problem is solved well? which algorithm(s) might be better suitable for this problem? Which technique(s) may help further improve the performance? \\

Last but not the least, don't forget to include references to any work you mentioned in the report.
  

\bibliographystyle{IEEEtran}
\bibliography{}


\end{document}


