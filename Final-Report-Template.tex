\documentclass[conference]{IEEEtran}
%\documentclass[sigconf]{acmart}
\makeatletter
\def\ps@headings{%
\def\@oddhead{\mbox{}\scriptsize\rightmark \hfil \thepage}%
\def\@evenhead{\scriptsize\thepage \hfil \leftmark\mbox{}}%
\def\@oddfoot{}%
\def\@evenfoot{}}
\makeatother
\pagestyle{empty}
\usepackage{url}
\usepackage{graphicx,subfigure}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{epsfig}
\newtheorem{theorem}{Theorem}
\renewcommand{\algorithmicrequire}{\textbf{Input:}} % Use Input in the format of Algorithm
\renewcommand{\algorithmicensure}{\textbf{Output:}} % Use Output in the format of Algorithm
\usepackage{amsfonts}
%\newtheorem{theorem}{Theorem}[section]
\newtheorem{mydef}{Definition}[section]
%\newtheorem{lemma}{Lemma}[section]
\usepackage{multirow}
\usepackage{color}
\usepackage{array}
\usepackage{listings}
\usepackage{hyperref}
\usepackage[underline=true]{pgf-umlsd}
\newcommand{\tabincell}[2]
{\begin{tabular}
		{@{}#1@{}}#2\end{tabular}}
\usepackage{setspace}
\renewcommand{\labelitemi}{$\vcenter{\hbox{\tiny$\bullet$}}$}


\hyphenation{op-tical net-works semi-conduc-tor}




\begin{document}



\title{Breast Cancer Wisconsin Dataset:\\ 
Application and Results of 6 Different Classification Algorithms}

\author{\IEEEauthorblockN{Sarah DeCelie}
\IEEEauthorblockA{\textit{Applied Machine Learning} \\
\textit{Stevens Institute of Technology}\\
Hoboken, USA \\
sdecelie1@stevens.edu}
\and
\IEEEauthorblockN{David Ogbonna}
\IEEEauthorblockA{\textit{Applied Machine Learning} \\
\textit{Stevens Institute of Technology}\\
Hoboken, USA \\
email address}
\and
\IEEEauthorblockN{Susan Tan}
\IEEEauthorblockA{\textit{Applied Machine Learning} \\
\textit{Stevens Institute of Technology}\\
Hoboken, USA \\
stan12@stevens.edu}
}

\maketitle


\begin{abstract}
One in 8 women in the United States is diagnosed with Breast Cancer. It has thus become pivotal in the field of healthcare to predict whether a cancer call is harmful as early as possible in order to treat patients sooner and save their lives. The Breast Cancer dataset, composed of 569 samples from Wisconsin, includes features of a breast cancer mass such as area, radius, texture, and whether the samples are malignant or benign. In this paper, we apply six different classification algorithms - Decision Tree Learning, Random Forest, SVM, kNN, Logistic Regression, and Naive Bayes - and compare their results to determine which performs the best finding a model that separates the malignant and benign samples.
\end{abstract}

\section{Introduction}
According to BreastCancer.org, breast cancer is the most common cancer across the globe, accounting for roughly 12.5\% of all new cancer cases worldwide. This is reflected within the United States, where an estimated 30\% of newly diagnosed cancers in women will be breast cancer \cite{BCorg}. It is no secret how lethal this cancer can be, and how critical it is to be able to detect it early in order to begin treatment and save patients' lives. Of course, genetics plays a role, which is why when one woman in a family is diagnosed with breast cancer, doctors will urge others that are closely related to her to get tested as well. However, the majority of breast cancer cases occur in women with no prior family history of breast cancer - about 85\% specifically \cite{BCorg}. With such a high incidence of mutations resulting from aging and other life processes, it becomes evident that relying solely on the diagnoses of other women in the family is not a perfect strategy. We must become better at detecting harmful breast cancer cells before they spread into the rest of the body.

For this study, we intend to apply six different classification algorithms to a dataset created by the University of Wisconsin, whose researchers took grayscale images of 569 different breast cancer cell's nuclei. This dataset consists of several features of each nucleus, and whether the cell is diagnosed malignant (M) or benign (B). Of these 569 samples, 357 were diagnosed benign, and 212 malignant. For each of these 10 features, the mean, standard error, and "worst" (mean of the three largest values) is calculated, totaling to 32 columns including the sample's unique ID and diagnosis. 

The six algorithms that will be applied to this dataset include: Decision Tree, Random Forest, k-Neural Network (kNN), Logistic Regression, Support Vector Machine (SVM), and Naive Bayes Classifier. These algorithms have been applied in previous studies, including those that used medical data, and the results have been promising. In particular, it appears the SVM and Random Forest perform the best, with kNN performing the worst, but none have testing accuracies below 90\%. Thus, there is high confidence that these algorithms will all perform very well with this dataset, further proving the viability of using machine learning to diagnose breast cancer.

\section{Related Work}
As stated previously, these algorithms have been utilized in several different previous works, including studies that focused on medical data. One such example is \textit{Machine Learning Algorithms For Breast Cancer Prediction and Diagnosis}, which actually applied 5 of the 6 algorithms that will be used in this study (SVM, Random Forest, Decision Tree, Logistic Regression, and kNN) to the same Breast Cancer Wisconsin dataset. The goals of this study were similar to this one: predict and diagnosis breast cancer using machine learning, and determine which of the selected algorithms is the most effective. Based on the resulting confusion matrix, testing accuracy and precision for each algorithm, it was observed that SVM performed the best with an accuracy of 97.2\%. Though, it should be noted that the other algorithms were not too far off, with a minimum testing accuracy of 93.7\% \cite{NAJI2021487}. These results are very promising, and raises curiosity now that Random Forests will be added to the list of tested algorithms.

Another study, \textit{Diagnosis of Breast Cancer Using Random Forests}, also used the same dataset and some of the same algorithms as this study. Specifically, this study compared the accuracies of SVM, Decision Tree, Multilayer Perceptron, kNN, and Random Forest. This study actually determined that Random Forest outperforms the other algorithms, with a perfect 100\% accuracy, precision, recall, F1 score, and ROC-AUC \cite{MINNOOR2023429}. With both of these studies, kNN seemed to perform the worst, but still with a good accuracy of around 93 to 94 percent.

The first study noted the fact that the use of one dataset, centering around breast cancer cells only, is a potential limitation. The second study added that the Wisconsin Breast Cancer dataset consists of calculations made from images taken of the cancerous cells' nuclei, and that developments in image processing may improve the performance of the algorithms with more accurate data. These drawbacks of using this dataset should be kept in mind, and the potential impacts on the results obtained later in this study will be discussed further.

\section{Our Solution}
This section elaborates your solution to the problem.

\subsection{Description of Dataset}
The Breast Cancer Wisconsin dataset is sourced from kaggle.com. It contains 569 samples: 357 benign and 212 malignant samples. Each sample is identified by an ID number and categorized as either M for malignant or B for benign. Ten real-valued features are computed for each cell nucleus/ sample: 
\begin{itemize}
	\item Radius (mean of distances from center to points on the perimeter
 	\item Texture (standard deviation of gray scale values)
  	\item Perimeter $p$
   	\item Area $a$
        \item Smoothness (local variation in radius lengths)
        \item Compactness ($p^2 / a - 1.0$)
        \item Concavity (severity of concave portions of the contour)
        \item Concave Points (number of concave portions of the contour)
	\item Symmetry
 	\item Fractal Dimension ("coastline approximation" - 1)
\end{itemize}
For each of the 10 feature above, the mean, standard error, and worst is calculated, totaling to 32 columns. Before applying machine learning algorithms to process this dataset and see how accurate each machine learning algorithm is at classifying a sample as benign or malignant, we first pre-processed the data. We read the csv file and got rid of the last column "Unnamed:32". We checked for NaN values, which apart for "Unnamed:32" column, this dataset did not have. After ensuring that the dataset contains only non-null values, we got rid of id column and separated the data into x (everything but diagnosis) and y(diagnosis). The data was then ready to be processed by our machine learning algorithms.

\subsection{Machine Learning Algorithms}
(delete later) This subsection describes machine learning algorithms that you plan to use. For each ML algorithm, briefly 1) explain why it might be appropriate for the problem and 2) describe your main design. For example, if it is neural network, provide the network structure and your initial choice of some key parameters (e.g., activation function to use, number of layers, number of hidden nodes of each layer). You may change the parameters during the training process. 
The six machine learning algorithms applied to the Breast Cancer Wisconsin Dataset are Decision Tree, Random Forest, k-Neural Network (kNN), Logistic Regression, Support Vector Machine (SVM), and Naive Bayes Classifier. In this section, we will explain each algorithm and debrief why we chose these six. 
1) Decision Tree : A Decision Tree is a non-parametric supervised learning algorithm that is used for both classification and regression tasks. It has a hierarchical, tree structure consisting of a root node, branches, internal nodes, and leaf nodes. Given an input sample, the root conducts evaluations using the given features, then decides which branch to move to next. The same thing happens in the following layer, whichever internal (decision) node the sample is moved to, until a leaf node is eventually reached with the final output decision. 
2) Random Forest : This is a common machine learning algorithm that combines the otuput of multiple decision trees to reach a single result. This is an ensemble learning method, which is made up of a set of classifiers (in this case, decision trees) and their predictions are aggregated to identify the most popular result. Single decision trees can be prone to problems such as bias and overfitting, so utilizing multiple decision trees to form an ensemble leads to more accurate results, especially when the trees are uncorrelated with each other.
3) k-Nearest Neighbors :
4) Logistic Regression :
5) Support Vector Machine : SVM aims to find the optimal hyperplanes that would separate two classes of data. The two classes in our problem are benign and malignant samples. Benign and malignant samples may be separable by their features such as area of mass, perimeter, and more. Hence, the SVM algorithm is a good choice for this problem set. The SVM algorithm models f(x) as \(f(x) = w^T*x + b\) and minimize or maximize the Lagrangian function to find the hyperplanes. The Lagrangian function is \(max L_D = \sum_{i=1}^{\l}a_i - 1/2\sum_{i=1}^{\l}\sum_{j=1}^{\l}a_i a_j y_i y_j(x_i^T \cdot x_j)\). A kernel function can be added in front of the dot product of the x if the classification problem is not linear. 
6) Naive Bayes : Naive Bayes finds the probability of a hypothesis (yes or no) by computing the probability of the set of attributes given the hypothesis. Naive Bayes assumes that the vector of attributes are independent of each other. In this problem set, the hypotheses are benign or malignant and the vector of attributes are the variations of the 10 features. Hence, Naive Bayes can be applied to predict whether the output is benign or malignant given the set of attributes. The Naive Bayes Classifier is \(c_NB = argmax_cj\inC P(c_j) \prod_i P(a_i |C_j)\). 

\subsection{Implementation Details}
This subsection describes details of your implementation. Please focus on how you test and validate the performance, tune the hyperparameters, and select the best-performing models. Elaborate on techniques that you apply to improve the performance and explain why you use these techniques. You include few most important results/figures to illustrate your idea but do not let figures/tables dominate the content of the report. You can include few lines of critical code if needed. But please avoid paste lengthy code in your report. Please make sure the figures/tables/code snapshots are of appropriate size including the font size.
In this section, we will describe how we implemented the 6 machine learning algorithms above. We will describe their performance and how we fine tuned their hyperparamters to get the best performance. 

\begin{enumerate}
    \item Decision Tree : The Decision Tree was implemented using the tree package of python's sklearn library. Decision Trees are not sensitive to outliers or normalization, so these techniques were not applied to the dataset. The ID column was dropped, as it would not be helpful for the algorithm. The raw data was imported from a .data file through the python library panda's .read_csv function, then the ID column was dropped as it would not aid the algorithm. Then the feature columns and target column (diagnosis) were separated, and then were further divided into an 80\% training and 20\% testing split.
    The Decision Tree was first trained with a min_samples_split parameter of 10. The testing accuracy of this tree was about 93.86\%, and the full tree is as shown below:
    [insert image here]
    The next step was to prune the tree. Using the GridSearchCV method, it was determined that the optimal values for max_leaf_nodes and min_samples_split were 46 and 5, respectively. The pruned tree then had a slightly improved accuracy of 94.74\%, and appeared as so:
    [insert image here]

    \item Random Forest : (to be implemented after mid-stage report)

    \item Logistic Regression : 

    \item k-Nearest Neighbors : (to be implemented after mid-stage report)

    \item Support Vector Machine : To implement support vector machine, the package svm was imported from python sklearn library. The function Support Vector Classifier SVC() was used as the model for fitting the training data (0.8 of dataset) and predicting the test data (0.2 of dataset). This model's performance was evaluated by printing the accuracy score, confusion matrix, and report. Using the SVC() model with its default parameters, the accuracy score was 0.98, the precision of predicting benign samples was 0.99, and the precision of predicting malignant samples was 0.98. There was only 1 FP for benign and 1 FN for malignant. 
	To test if this was the optimal performance by the SVC model, this performance was tested against using GridSearchCV() to find optimal values for the SVM parameters - C, gamma, and kernel function. The popular kernel function "rbf" or radial basis function was one of the GridSearchCV() parameters. This kernel function is good for models that are linearly inseparable or non-linear. The C value in SVM is the penalty parameter. A smaller C results in wider margin but more misclassified examples. A larger C results in smaller margin and less misclassified examples, but more overfitting. Gamma is the inverse of radius of influence. Gamma is also another SVM parameter added to GridSearchCV(). If gamma is too smaell, the model does not copture the shape of data. After applying GridSearchCV to a range of values for C and gamma, as well as using the popular kernel function "rbf", the optimal values of C and gamma were 10 and 1 respectively. 
	The optimal values for the parameters found through GridSearchCV() was then applied to the SVC() model. This new model was used to fit the training data and predict the test data. The performance of the SVC model with C of 10 and gamma of 1 once again had a precision of 0.99 for predicting benign samples and 0.98 for predicting malignant samples. This result was the same as using the default values for SVC(). 

    \item Naive Bayes : (To be implemented after mid-stage report)
\end{enumerate}



\section{Comparison}  
This section includes the following: 1) comparing the performance of different machine learning algorithms that you used, and 2) comparing the performance of your algorithms with existing solutions if any. Please provide insights to reason about why this algorithm is better/worse than another one.

\section{Future Directions}
This section lays out some potential directions for further improving the performance. You can image what you may do if you were given extra 3-6 months.

\section{Conclusion}
This section summarizes this project, i.e., by the extensive experiments and analysis, do you think the problem is solved well? which algorithm(s) might be better suitable for this problem? Which technique(s) may help further improve the performance? \\

Last but not the least, don't forget to include references to any work you mentioned in the report.
  

\bibliographystyle{IEEEtran}
\bibliography{Latext-Template/bib}


\end{document}
